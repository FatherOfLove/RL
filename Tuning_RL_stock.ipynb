{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tuning_RL_stock.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatherOfLove/RL/blob/main/Tuning_RL_stock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-YXowCUPk4c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVtV_o6QPwFB"
      },
      "source": [
        "stock_data1 = pd.read_csv(('/gdrive/My Drive/Data set/Stock/three_years/VCYT.csv'))\n",
        "stock_data2 =  pd.read_csv(('/gdrive/My Drive/Data set/Stock/three_years/AMD.csv'))\n",
        "stock_data3 = pd.read_csv(('/gdrive/My Drive/Data set/Stock/three_years/AAPL.csv'))\n",
        "stock_data4 = pd.read_csv(('/gdrive/My Drive/Data set/Stock/three_years/NSTG.csv'))\n",
        "stock_data5 = pd.read_csv(('/gdrive/My Drive/Data set/Stock/NIO.csv'))\n",
        "stock_data6 = pd.read_csv(('/gdrive/My Drive/Data set/Stock/three_years/JYNT.csv'))\n",
        "stock_data7 = pd.read_csv(('/gdrive/My Drive/Data set/Stock/three_years/NQSDA.csv'))\n",
        "stock_data8 = pd.read_csv(('/gdrive/My Drive/Data set/Stock/three_years/KODK.csv'))\n",
        "stock_data9 = pd.read_csv(('/gdrive/My Drive/Data set/Stock/YALA.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST6YR0krJvv3"
      },
      "source": [
        "NQSDA_Index =  [] \n",
        "\n",
        "stock_data = stock_data7[stock_data7[\"Date\"].isin(stock_data8.Date.values)]\n",
        "amount_stock_NQSDA = 100000/stock_data7.Close[0]\n",
        "for price in stock_data7['Close'].tolist() : \n",
        "  NQSDA_Index.append(amount_stock_NQSDA *price)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5eqk5oMPwHU"
      },
      "source": [
        "df = stock_data4\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVUxQq4WPwOB"
      },
      "source": [
        "plt.figure(figsize=(15, 5), dpi=100)\n",
        "plt.plot(df['Date'], df['Close'], color='black')\n",
        "plt.xticks(np.linspace(0, len(df), 10))\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuVttD7_UU6E"
      },
      "source": [
        "df=df.sort_values('Date', ascending=True).reset_index(drop=True)\n",
        "\n",
        "# caculate MA（moving avarage price）\n",
        "for day in [5, 10, 21, 30]:\n",
        "    df['ma'+str(day)] = df.Close.rolling(day).mean()\n",
        "\n",
        "col = ['ma5', 'ma10', 'ma21', 'ma30']\n",
        "tmp_df2 = df[col].shift(1)\n",
        "tmp_df3 = df[col].shift(2)\n",
        "for tmp_col in col:\n",
        "    df[tmp_col + '_trend'] = 0\n",
        "    # tmp_df[tmp_col + '_shift_1'] = tmp_df2[tmp_col]\n",
        "    # tmp_df[tmp_col _ 'shift_2'] = tmp_df3[tmp_col]\n",
        "    df['rate1'] = (tmp_df2[tmp_col] - tmp_df3[tmp_col]) / (tmp_df3[tmp_col] + 0.00001)\n",
        "    df['rate2'] = (df[tmp_col] - tmp_df2[tmp_col]) / (tmp_df2[tmp_col] + 0.00001)\n",
        "    idx = (df['rate1'] > 0.005) & (df['rate2'] > 0.005)\n",
        "    df.loc[idx, tmp_col + '_trend'] = 1 # trend going up\n",
        "\n",
        "    idx = (df['rate1'] < -0.005) & (df['rate2'] < -0.005)\n",
        "    df.loc[idx, tmp_col + '_trend'] = 2 # # trend going down\n",
        "\n",
        "\n",
        "\n",
        "df = df.loc[31:].reset_index(drop=True)\n",
        "df_copy = df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6_AA8gjktHh"
      },
      "source": [
        "df_copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9atQNpYn1oeP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brdaTj83V3m1"
      },
      "source": [
        "#todo: Position control strategy\n",
        "# https://www.investopedia.com/articles/trading/10/create-trading-strategies.asp\n",
        "\n",
        "def compute_score(index_df, i):\n",
        "    score = 0\n",
        "    tmp_rate = (index_df['Close'][i]-index_df['ma5'][i])\n",
        "    if tmp_rate>=0:\n",
        "        score += 1\n",
        "        \n",
        "    tmp_rate = (index_df['Close'][i]-index_df['ma10'][i])\n",
        "    if tmp_rate>=0:\n",
        "        score += 1.5\n",
        "    elif tmp_rate>-0.03:\n",
        "        score += 1\n",
        "    elif tmp_rate>-0.06:\n",
        "        score += 0.5\n",
        "    \n",
        "    tmp_rate = (index_df['Close'][i]-index_df['ma21'][i])\n",
        "    if tmp_rate>=0:\n",
        "        score += 1.5\n",
        "    elif tmp_rate>-0.03:\n",
        "        score += 1\n",
        "    elif tmp_rate>-0.06:\n",
        "        score += 0.5\n",
        "\n",
        "    tmp_rate = (index_df['Close'][i]-index_df['ma30'][i])\n",
        "    if tmp_rate>=0:\n",
        "        score += 1\n",
        "    elif tmp_rate>-0.5:\n",
        "        score += 0.5\n",
        "    \n",
        "    if index_df['ma5_trend'][i] == 1:\n",
        "        score += 1\n",
        "    \n",
        "    if index_df['ma10_trend'][i] == 1:\n",
        "        score += 1.5\n",
        "    elif index_df['ma10_trend'][i] == 0:\n",
        "        score += 0.5\n",
        "\n",
        "    if index_df['ma21_trend'][i] == 1:\n",
        "        score += 1.5\n",
        "    elif index_df['ma21_trend'][i] == 0:\n",
        "        score += 0.5\n",
        "        \n",
        "    if index_df['ma21_trend'][i] == 1:\n",
        "        score += 1\n",
        "    elif index_df['ma21_trend'][i] == 0:\n",
        "        score += 0.5\n",
        "        \n",
        "    return score\n",
        "\n",
        "# Manual strategy\n",
        "\n",
        "def print_info(hold_money, hold_market_value, index_df, i, kind, change_amount):\n",
        "    trade_date = index_df['Date'][i]\n",
        "    change_amount = round(change_amount,2)\n",
        "    rate = (index_df['Close'][i] - index_df['Close'][i-1]) / index_df['Close'][i-1]\n",
        "    rate = round(rate, 2)\n",
        "    info = str(trade_date)\n",
        "    if rate > 0:\n",
        "        info += ', going up ' + str(rate)\n",
        "    else:\n",
        "        info += ', going down ' + str(rate)\n",
        "        \n",
        "    if kind == 1:\n",
        "        info += ', buy ' + str(change_amount) \n",
        "    \n",
        "    elif kind == 2:\n",
        "        info += ', sell ' + str(change_amount) \n",
        "    \n",
        "    info += ',hold money：'+str(round(hold_money,2))+',stocks_value：'+\\\n",
        "    str(round(hold_market_value,2))+',total_market_value：'+str(round(hold_money+hold_market_value,2))\n",
        "    print(info)\n",
        "\n",
        "\n",
        "score_list = []\n",
        "hold_mount = 0\n",
        "money_init = 100000\n",
        "hold_money = money_init\n",
        "hold_market_value = 0\n",
        "hold_market_value_list = []\n",
        "hold_money_list = []\n",
        "all_value_list_manual = []\n",
        "change_amount = 0\n",
        "for i in range(len(df)):\n",
        "    score = compute_score(df, i) / 10    \n",
        "    #print(str(score))\n",
        "    if i == 0:\n",
        "        buy_money = int(money_init * score)\n",
        "        hold_market_value = buy_money\n",
        "        hold_money = hold_money - buy_money\n",
        "        print(str(df['Date'][i])+'buy：'+str(str(buy_money)))\n",
        "        print(hold_market_value)\n",
        "    else:\n",
        "        hold_market_value = hold_market_value * df['Close'][i]/df['Close'][i-1]\n",
        "        print(hold_market_value)\n",
        "        if score_list[-1] == score:\n",
        "            kind = 0\n",
        "            \n",
        "        elif score_list[-1] < score: #buy\n",
        "            if score_list[-1] == 0:\n",
        "                add_mount = score\n",
        "            else:\n",
        "                add_mount = score - score_list[-1]\n",
        "            \n",
        "            buy_money = int(hold_money/(1-score_list[-1])* (add_mount))\n",
        "            hold_market_value += buy_money\n",
        "            hold_money -= buy_money\n",
        "            kind = 1\n",
        "            change_amount = add_mount\n",
        "            \n",
        "        else: #sell\n",
        "            reduce_mount =  score_list[-1] - score\n",
        "            sell_money = int(hold_market_value/score_list[-1] * (reduce_mount))\n",
        "            hold_market_value -= sell_money\n",
        "            hold_money += sell_money\n",
        "            change_amount = sell_money\n",
        "            kind = 2\n",
        "            \n",
        "        print_info(hold_money, hold_market_value, df, i, kind, change_amount) \n",
        "        \n",
        "    score_list.append(score)\n",
        "    hold_market_value_list.append(hold_market_value)\n",
        "    hold_money_list.append(hold_money)\n",
        "    all_value_list_manual.append(hold_money+hold_market_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXd-muyQV0Dl"
      },
      "source": [
        "# Q-training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ6kMOSzPwQU"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, state_size, window_size, trend, frequency, batch_size , gamma = 0.98,epsilon = 0.5, epsilon_min = 0.03,epsilon_decay = 0.999, lr =0.00001):\n",
        "        self.state_size = state_size #size of the state\n",
        "        self.window_size = window_size #window size, buy stop before the window size days\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend  #data\n",
        "        self.frequency = frequency \n",
        "        self.action_size = 3 #there are three action: buy, sell and hold\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = deque(maxlen = 1000) \n",
        "        self.inventory = [] \n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "\n",
        "        tf.compat.v1.reset_default_graph()\n",
        "        #tf.reset_default_graph()\n",
        "        self.sess = tf.compat.v1.InteractiveSession()\n",
        "        tf.compat.v1.disable_eager_execution()\n",
        "        #self.sess = tf.InteractiveSession() \n",
        "        self.X = tf.compat.v1.placeholder(tf.float32, [None, self.state_size]) #state\n",
        "        self.Y = tf.compat.v1.placeholder(tf.float32, [None, self.action_size]) #action\n",
        "        #print(self.X)\n",
        "        feed = tf.keras.layers.Dense(256, activation ='relu')(self.X)\n",
        "        feed = tf.keras.layers.Dense(128, activation ='relu')(self.X)\n",
        "        feed = tf.keras.layers.Dense(64, activation ='relu')(self.X)\n",
        "        feed = tf.keras.layers.Dense(32, activation ='relu')(self.X)\n",
        "        feed = tf.keras.layers.Dense(16, activation ='relu')(self.X)\n",
        "        feed = tf.keras.layers.Dense(8, activation ='relu')(self.X)\n",
        "        self.prob = tf.keras.layers.Dense( self.action_size)(feed) #calulate the prob of 3 different action\n",
        "        self.loss = tf.reduce_mean(input_tensor=tf.square(self.Y - self.prob)) #calculate the loss function\n",
        "        self.optimizer = tf.compat.v1.train.GradientDescentOptimizer(lr).minimize(self.loss) \n",
        "        self.total_money_list=[]\n",
        "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    def act(self, state, is_test=False): #choose action\n",
        "        if not is_test:\n",
        "          if random.random() <= self.epsilon: #if less than epsilon will use random range\n",
        "            return random.randrange(self.action_size)\n",
        "\t\t#else choose the best action\n",
        "        return np.argmax( \n",
        "            self.sess.run(self.prob, feed_dict = {self.X: state})[0]\n",
        "        )\n",
        "    \n",
        "    def get_state(self, t): #get the state in time t\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "\t\t    #initial window size not enough, use 0 instead of window size\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i]) #reward of each step\n",
        "        return np.array([res]) #assign state \n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        mini_batch = []\n",
        "        l = len(self.memory)\n",
        "        for i in range(l - batch_size, l):\n",
        "            mini_batch.append(self.memory[i])#memory\n",
        "        replay_size = len(mini_batch)\n",
        "        X = np.empty((replay_size, self.state_size))\n",
        "        Y = np.empty((replay_size, self.action_size))\n",
        "        \n",
        "\t\t#Q calulate\n",
        "\t\t#[state, action, reward, next_state, done]\n",
        "\t\t# initial Q table\n",
        "        states = np.array([a[0][0] for a in mini_batch])\n",
        "        new_states = np.array([a[3][0] for a in mini_batch])\n",
        "        Q = self.sess.run(self.prob, feed_dict = {self.X: states})\n",
        "        Q_new = self.sess.run(self.prob, feed_dict = {self.X: new_states})\n",
        "\t\t#update Q table\n",
        "        for i in range(len(mini_batch)):\n",
        "            state, action, reward, next_state, done = mini_batch[i]\n",
        "            target = Q[i]\n",
        "            target[action] = reward\n",
        "            if not done: \n",
        "                target[action] += self.gamma * np.amax(Q_new[i])\n",
        "\t\t\t\n",
        "            X[i] = state\n",
        "            Y[i] = target\n",
        "        loss, _ = self.sess.run(\n",
        "            [self.loss, self.optimizer], feed_dict = {self.X: X, self.Y: Y}\n",
        "        )\n",
        "\t\t#adjust the epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        #print(Q)\n",
        "        return loss\n",
        "\n",
        "    def test(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = [] \n",
        "        market_value = initial_money\n",
        "        hold_money = initial_money\n",
        "        stock_value = 0\n",
        "        hold_num = 0\n",
        "        self.total_money_list.append(initial_money)\n",
        "        state = self.get_state(0) \n",
        "        for t in range(0, len(self.trend) - 1):\n",
        "            if t%self.frequency!=0 :\n",
        "              stock_value = self.trend[t] * hold_num *100\n",
        "              market_value = stock_value + hold_money \n",
        "              self.total_money_list.append(market_value)\n",
        "              continue                           \n",
        "            action = self.act(state, True) #do action base on state\n",
        "            next_state = self.get_state(t + 1) #get next state\n",
        "            #action=1 buy\n",
        "            if action == 1 and hold_money >= self.trend[t]*100 and t < (len(self.trend) - self.half_window):\n",
        "                inventory.append(self.trend[t]) #buy\n",
        "                #initial_money -= self.trend[t] \n",
        "                buy_num = int(hold_money // self.trend[t] // 100)\n",
        "                hold_num += buy_num\n",
        "                stock_value += self.trend[t]*100 * buy_num\n",
        "                hold_money -= self.trend[t]*100 * buy_num\n",
        "\n",
        "                states_buy.append(t) #record the transcation \n",
        "                print('day %d: buy %d unit at price %f'% (t,buy_num, self.trend[t]))\n",
        "                \n",
        "            #action=2 sell\n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0) #sell\n",
        "                #initial_money += self.trend[t] \n",
        "                states_sell.append(t) #record the transcation \n",
        "                hold_money += self.trend[t] * hold_num * 100          \n",
        "                stock_value =0  \n",
        "                market_value = stock_value + hold_money\n",
        "              \t#calculate the invest\n",
        "                try: \n",
        "                    invest = ((self.trend[t] - bought_price) / bought_price) \n",
        "                except:\n",
        "                    invest = 0\n",
        "                    \n",
        "                print(\n",
        "                    'day %d, sell %d unit at price %f, investment %f %%, total balance %f,'\n",
        "                      % (t, hold_num, self.trend[t], invest, market_value)\n",
        "                )\n",
        "                hold_num = 0\n",
        "            \n",
        "            stock_value = self.trend[t] * hold_num *100\n",
        "            market_value = stock_value + hold_money\n",
        "            total_profit = market_value - initial_money\n",
        "            #invest = ((market_value - initial_money) / initial_money)\n",
        "\n",
        "            # self.memory.append((state, action, invest, \n",
        "            #                         next_state, starting_money < initial_money))\n",
        "                                 \n",
        "            self.total_money_list.append(market_value)\n",
        "            state = next_state \n",
        "\n",
        "\t\t#calculate the invest\n",
        "        invest_all = ((market_value - starting_money) / starting_money)* 100\n",
        "        total_gains = market_value - starting_money\n",
        "        \n",
        "        return states_buy, states_sell, total_gains, invest_all\n",
        "        \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "\t\t\n",
        "        for i in range(iterations):\n",
        "            #print(str(i))\n",
        "            total_profit = 0 \n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            last_value = initial_money\n",
        "            market_value = initial_money\n",
        "            hold_money = initial_money\n",
        "            stock_value = 0\n",
        "            hold_num = 0\n",
        "\n",
        "            for t in range(0, len(self.trend) - 1, self.frequency):\n",
        "                #print(str(i),t)\n",
        "                action = self.act(state)\n",
        "                next_state = self.get_state(t + 1)\n",
        "                #print(next_state)\n",
        "                if action == 1 and hold_money >= self.trend[t]*100 and t < (len(self.trend) - self.half_window):\n",
        "                    inventory.append(self.trend[t])\n",
        "                    buy_num = int(hold_money// self.trend[t] // 100)#buy stock number\n",
        "                    hold_num += buy_num\n",
        "                    stock_value += self.trend[t]*100 * buy_num\n",
        "                    hold_money -= self.trend[t]*100 * buy_num\n",
        "                    #print(stock_value,hold_money)\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    # bought_price = inventory.pop(0)\n",
        "                    # total_profit += self.trend[t] - bought_price\n",
        "                    # starting_money += self.trend[t]\n",
        "                    hold_money += self.trend[t]* hold_num*100 \n",
        "                    hold_num = 0\n",
        "                    stock_value = 0\n",
        "                    #print(stock_value,hold_money)\n",
        "                    \n",
        "                stock_value = self.trend[t] * hold_num *100\n",
        "                market_value = stock_value + hold_money\n",
        "                total_profit = market_value - initial_money\n",
        "                invest = ((market_value - last_value) / last_value)\n",
        "                last_value = market_value\n",
        "                #print(stock_value,market_value,total_profit,invest,last_value)\n",
        "\n",
        "                self.memory.append((state, action, invest, \n",
        "                                    next_state, starting_money < initial_money))\n",
        "                state = next_state\n",
        "                #print(self.batch_size)\n",
        "                batch_size = min(self.batch_size, len(self.memory))\n",
        "                #print(batch_size)\n",
        "                loss = self.replay(batch_size)\n",
        "                #print(loss)\n",
        "            \n",
        "            if (i+1) % checkpoint == 0:\n",
        "                invest_all = ((market_value - initial_money) / initial_money)\n",
        "                #if invest_all > 1.0:\n",
        "                #    break  \n",
        "                print('epoch: %d, total rewards: %f, loss: %f, total money: %f'%(i + 1, total_profit, loss, market_value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CltdE4tglAOg"
      },
      "source": [
        "def back_test(df,initial_money,window_size,frequency,batch_size,iterations,checkpoint,gamma = 0.98,epsilon = 0.5, epsilon_min = 0.03,epsilon_decay = 0.999, lr =0.02): \n",
        "    trend = df.Close.values.tolist() #using close price to test  \n",
        "    agent_Q = Agent(state_size = window_size, \n",
        "                  window_size = window_size, \n",
        "                  trend = trend, \n",
        "                  frequency = frequency, \n",
        "                  batch_size = batch_size,\n",
        "                  gamma = gamma,\n",
        "                  epsilon = epsilon, \n",
        "                  epsilon_min = epsilon_min,\n",
        "                  epsilon_decay = epsilon_decay, \n",
        "                  lr = lr)\n",
        "    agent_Q.train(iterations = iterations, checkpoint = checkpoint, initial_money = initial_money)\n",
        "       \n",
        "\n",
        "    states_buy, states_sell, total_gains, invest = agent_Q.test(initial_money = initial_money)\n",
        "    \n",
        "    \n",
        "   \n",
        "    fig = plt.figure(figsize = (15,5))\n",
        "    plt.plot(trend, color='r', lw=2.)\n",
        "    plt.plot(trend, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "    plt.plot(trend, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "    plt.title('gamma: %.3f,epsilon: %.3f,epsilon_min: %.3f,epsilon_decay: %.3f,lr: %.6f\\n'%(gamma,epsilon,epsilon_min,epsilon_decay,lr)+'total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "     #buy first day and hold strategy\n",
        "    buy_hold_strategy_value =  [] \n",
        "    amount_stock = initial_money/trend[0]\n",
        "    for price in trend : \n",
        "      buy_hold_strategy_value.append(amount_stock *price)\n",
        "    #print(len(buy_hold_strategy_value))\n",
        "    #print(len(agent.total_money_list))\n",
        "\n",
        "    fig = plt.figure(figsize = (15,5))\n",
        "    plt.plot(buy_hold_strategy_value, color='r', lw=2.,label=\"buy_hold_strategy%.1f\"%(buy_hold_strategy_value[-1]))\n",
        "    plt.plot(all_value_list_manual, color='y', lw=2.,label=\"Manual strategy%.1f\"%(all_value_list_manual[-1]))\n",
        "    plt.plot(agent_Q.total_money_list, color='b', lw=2.,label=\"QL_agent_value%.1f\"%(agent_Q.total_money_list[-1]))\n",
        "    #plt.plot(NQSDA_Index, color='g', lw=2.,label=\"NQSDA_value\" )\n",
        "    plt.plot()\n",
        "    plt.ylabel(\"total value\")\n",
        "    plt.xlabel(\"day\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpeQG1cTaeN2"
      },
      "source": [
        "back_test(df,initial_money = 100000, window_size = 5, frequency = 1, batch_size = 32,iterations = 10, checkpoint = 10,gamma = 0.98,epsilon = 0.3, epsilon_min = 0.03,epsilon_decay = 0.999, lr =0.00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKVHFniOMAtQ"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "np.random.seed(1)\r\n",
        "torch.manual_seed(41)\r\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "device = 'cpu'\r\n",
        "\r\n",
        "class DQN(nn.Module):\r\n",
        "    def __init__(self, input_shape, n_actions):\r\n",
        "        super(DQN, self).__init__()\r\n",
        "        units = 32\r\n",
        "        self.fc1 = nn.Linear(input_shape, units)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.fc2 = nn.Linear(units, n_actions)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.fc1(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        x = self.fc2(x)\r\n",
        "        return x\r\n",
        "    \r\n",
        "# Deep Q Network off-policy\r\n",
        "class AgentDQN:\r\n",
        "    def __init__(\r\n",
        "            self,\r\n",
        "            n_actions,\r\n",
        "            n_features,\r\n",
        "            learning_rate=0.01,\r\n",
        "            reward_decay=0.9,\r\n",
        "            e_greedy=0.9,\r\n",
        "            replace_target_iter=300,\r\n",
        "            memory_size=500,\r\n",
        "            batch_size=32,\r\n",
        "            e_greedy_increment=None,\r\n",
        "            output_graph=False,\r\n",
        "    ):\r\n",
        "        self.n_actions = n_actions\r\n",
        "        self.n_features = n_features\r\n",
        "        self.lr = learning_rate\r\n",
        "        self.gamma = torch.tensor(reward_decay, dtype=torch.float)\r\n",
        "        self.epsilon_max = e_greedy\r\n",
        "        self.replace_target_iter = replace_target_iter\r\n",
        "        self.memory_size = memory_size\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.epsilon_increment = e_greedy_increment\r\n",
        "        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\r\n",
        "\r\n",
        "        # total learning step\r\n",
        "        self.learn_step_counter = 0\r\n",
        "\r\n",
        "        # initialize zero memory [s, a, r, s_]\r\n",
        "        self.memory = np.zeros((self.memory_size, n_features * 2 + 2))\r\n",
        "        \r\n",
        "        self.net = DQN(n_features, n_actions).to(device)\r\n",
        "        self.tgt_net = DQN(n_features, n_actions).to(device)\r\n",
        "        \r\n",
        "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\r\n",
        "\r\n",
        "\r\n",
        "    def inventory(self, s, a, r, s_):\r\n",
        "        if not hasattr(self, 'memory_counter'):\r\n",
        "            self.memory_counter = 0\r\n",
        "\r\n",
        "        transition = np.hstack((s, [a, r], s_))\r\n",
        "\r\n",
        "        # replace the old memory with new memory\r\n",
        "        index = self.memory_counter % self.memory_size\r\n",
        "        self.memory[index, :] = transition\r\n",
        "\r\n",
        "        self.memory_counter += 1\r\n",
        "\r\n",
        "    def choose_action(self, observation, train=True):\r\n",
        "        # to have batch dimension when feed into tf placeholder\r\n",
        "        observation = [observation[np.newaxis, :]]\r\n",
        "        observation = torch.tensor(observation, dtype=torch.float32).to(device)\r\n",
        "        \r\n",
        "        if np.random.uniform() < self.epsilon or (train == False):\r\n",
        "            # forward feed the observation and get q value for every actions\r\n",
        "            actions_value = self.net(observation).detach().cpu().squeeze(0)\r\n",
        "            action = np.argmax(actions_value)\r\n",
        "        else:\r\n",
        "            action = np.random.randint(0, self.n_actions)\r\n",
        "        return action\r\n",
        "\r\n",
        "    def train(self):\r\n",
        "        self.net.train()\r\n",
        "        self.tgt_net.train()\r\n",
        "        # check to replace target parameters\r\n",
        "        if self.learn_step_counter % self.replace_target_iter == 0:\r\n",
        "            self.tgt_net.load_state_dict(self.net.state_dict())\r\n",
        "            # print('\\ntarget_params_replaced\\n')\r\n",
        "            \r\n",
        "        # sample batch memory from all memory\r\n",
        "        if self.memory_counter > self.memory_size:\r\n",
        "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\r\n",
        "        else:\r\n",
        "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\r\n",
        "        \r\n",
        "        batch_memory = self.memory[sample_index, :]\r\n",
        "        s_ = torch.tensor(batch_memory[:, -self.n_features:], dtype=torch.float32).to(device)\r\n",
        "        s = torch.tensor(batch_memory[:, :self.n_features], dtype=torch.float32).to(device)\r\n",
        "        eval_act_index = batch_memory[:, self.n_features].astype(int)\r\n",
        "        reward = torch.tensor(batch_memory[:, self.n_features + 1], dtype=torch.float32)\r\n",
        "        \r\n",
        "        q_next = self.tgt_net(s_)\r\n",
        "        q_eval = self.net(s)\r\n",
        "\r\n",
        "        # # change q_target w.r.t q_eval's action\r\n",
        "        q_target = q_eval.clone()\r\n",
        "\r\n",
        "        batch_index = np.arange(self.batch_size, dtype=np.int32)\r\n",
        "        max_, _ = torch.max(q_next, dim=1)\r\n",
        "        q_target[batch_index, eval_act_index] = reward + self.gamma * max_\r\n",
        "        \r\n",
        "        # loss backpropogation\r\n",
        "        self.optimizer.zero_grad()\r\n",
        "        loss = nn.MSELoss()(q_eval, q_target)\r\n",
        "#        print(loss)\r\n",
        "        loss.backward()\r\n",
        "        self.optimizer.step()\r\n",
        "        \r\n",
        "        # increasing epsilon\r\n",
        "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\r\n",
        "        self.learn_step_counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avN_Ynj4za48"
      },
      "source": [
        "#Env\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "class stock_env:\r\n",
        "    \r\n",
        "    def __init__(self, df, init_money=100000, window_size=6):\r\n",
        "        \r\n",
        "        self.n_actions = 3 \r\n",
        "        self.n_features = window_size   \r\n",
        "        self.trend = df['Close'].values # close price\r\n",
        "        self.df = df #dataset\r\n",
        "        self.init_money = init_money \r\n",
        "        \r\n",
        "        self.window_size = window_size \r\n",
        "        self.half_window = window_size // 2\r\n",
        "        \r\n",
        "        self.buy_rate = 0.0003  \r\n",
        "        self.buy_min = 5  # min buying fee\r\n",
        "        self.sell_rate = 0.0003  # selling fee\r\n",
        "        self.sell_min = 5  # max buying fee\r\n",
        "        self.stamp_duty = 0.001  # tax\r\n",
        "        #print(len(self.trend))\r\n",
        "    def reset(self):\r\n",
        "        self.hold_money = self.init_money \r\n",
        "        self.buy_num = 0 \r\n",
        "        self.hold_num = 0 \r\n",
        "        self.stock_value = 0 \r\n",
        "        self.market_value = 0 \r\n",
        "        self.last_value = self.init_money # last day market value\r\n",
        "        self.total_profit = 0 # total profit\r\n",
        "        self.t = self.window_size // 2 # time\r\n",
        "        self.reward = 0 # reward\r\n",
        "        # self.inventory = []\r\n",
        "        \r\n",
        "        self.states_sell = [] \r\n",
        "        self.states_buy = [] \r\n",
        "        \r\n",
        "        self.profit_rate_account = [] \r\n",
        "        self.market_value_list = [] \r\n",
        "        self.profit_rate_stock = [] \r\n",
        "        return self.get_state(self.t)\r\n",
        "    \r\n",
        "    def get_state(self, t): #get state at t time\r\n",
        "        \r\n",
        "        window_size = self.window_size + 1\r\n",
        "        d = t - window_size + 1\r\n",
        "\t\t\r\n",
        "        # block = self.trend[d : t + 1] if d >= 0 else (-d * [self.trend[0]] + self.trend[0 : t + 1])\r\n",
        "        block = []\r\n",
        "        if d<0:\r\n",
        "            for i in range(-d):\r\n",
        "                block.append(self.trend[0])\r\n",
        "            for i in range(t+1):\r\n",
        "                block.append(self.trend[i])\r\n",
        "        else:\r\n",
        "            block = self.trend[d : t + 1]\r\n",
        "                \r\n",
        "            \r\n",
        "        res = []\r\n",
        "        for i in range(window_size - 1):\r\n",
        "            res.append((block[i + 1] - block[i])/(block[i]+0.0001)) #每步收益\r\n",
        "        # res = []\r\n",
        "            \r\n",
        "        # if self.hold_num > 0:\r\n",
        "        #     res.append(1)\r\n",
        "        # else:\r\n",
        "        #     res.append(0)\r\n",
        "            \r\n",
        "        # res.append((self.df['close'][t] - self.df['ma21'][t]) / self.df['ma21'][t])\r\n",
        "        # res.append((self.df['close'][t] - self.df['ma13'][t]) / self.df['ma13'][t])\r\n",
        "        # res.append((self.df['close'][t] - self.df['ma5'][t]) / self.df['ma5'][t])\r\n",
        "        # res.append((self.df['vol'][t] - self.df['ma_v_21'][t]) / self.df['ma_v_21'][t])\r\n",
        "        return np.array(res) #states \r\n",
        "    \r\n",
        "    def buy_stock(self):       \r\n",
        "        # buy\r\n",
        "        self.buy_num = self.hold_money // self.trend[self.t] // 100 # 买入手数\r\n",
        "        self.buy_num = self.buy_num * 100\r\n",
        "        \r\n",
        "        # fee\r\n",
        "        tmp_money = self.trend[self.t] * self.buy_num\r\n",
        "        service_change = tmp_money * self.buy_rate\r\n",
        "        if service_change < self.buy_min:\r\n",
        "            service_change = self.buy_min\r\n",
        "        \r\n",
        "        if service_change + tmp_money > self.hold_money:\r\n",
        "            self.buy_num = self.buy_num - 100\r\n",
        "        tmp_money = self.trend[self.t] * self.buy_num\r\n",
        "        service_change = tmp_money * self.buy_rate\r\n",
        "        if service_change < self.buy_min:\r\n",
        "            service_change = self.buy_min\r\n",
        "            \r\n",
        "        self.hold_num += self.buy_num\r\n",
        "        self.stock_value += self.trend[self.t] * self.buy_num\r\n",
        "        self.hold_money = self.hold_money - self.trend[self.t] * \\\r\n",
        "            self.buy_num - service_change\r\n",
        "        self.states_buy.append(self.t)\r\n",
        "    \r\n",
        "    def sell_stock(self, sell_num):\r\n",
        "        tmp_money = sell_num * self.trend[self.t]\r\n",
        "        service_change = tmp_money * self.sell_rate\r\n",
        "        if service_change < self.sell_min:\r\n",
        "            service_change = self.sell_min\r\n",
        "        stamp_duty = self.stamp_duty * tmp_money\r\n",
        "        self.hold_money = self.hold_money + tmp_money - service_change - stamp_duty\r\n",
        "        self.hold_num = 0\r\n",
        "        self.stock_value = 0\r\n",
        "        self.states_sell.append(self.t)\r\n",
        "        \r\n",
        "    def trick(self):\r\n",
        "        if self.df['Close'][self.t] >= self.df['ma21'][self.t]:\r\n",
        "            return True\r\n",
        "        else:\r\n",
        "            return False\r\n",
        "    \r\n",
        "    def step(self, action, show_log=False, my_trick=False):\r\n",
        "        \r\n",
        "        if action == 1 and self.hold_money >= (self.trend[self.t]*100 + \\\r\n",
        "            max(self.buy_min, self.trend[self.t]*100*self.buy_rate)) and self.t < (len(self.trend) - self.half_window):\r\n",
        "            buy_ = True\r\n",
        "            if my_trick and not self.trick(): \r\n",
        "                # ma filter\r\n",
        "                buy_ = False\r\n",
        "            if buy_ : \r\n",
        "                self.buy_stock()\r\n",
        "                if show_log:\r\n",
        "                    print('day:%d, buy price:%f, buy num:%d, hold num:%d, hold money:%.3f'% \\\r\n",
        "                          (self.t, self.trend[self.t], self.buy_num, self.hold_num, self.hold_money))\r\n",
        "        \r\n",
        "        elif action == 2 and self.hold_num > 0:\r\n",
        "            # selling stock         \r\n",
        "            self.sell_stock(self.hold_num)\r\n",
        "            if show_log:\r\n",
        "                print(\r\n",
        "                    'day:%d, sell price:%f, total balance %f,'\r\n",
        "                    % (self.t, self.trend[self.t], self.hold_money)\r\n",
        "                )\r\n",
        "        else:\r\n",
        "            if my_trick and self.hold_num>0 and not self.trick():\r\n",
        "                self.sell_stock(self.hold_num)\r\n",
        "                if show_log:\r\n",
        "                    print(\r\n",
        "                        'day:%d, sell price:%f, total balance %f,'\r\n",
        "                        % (self.t, self.trend[self.t], self.hold_money)\r\n",
        "                    )\r\n",
        "                    \r\n",
        "        self.stock_value = self.trend[self.t] * self.hold_num\r\n",
        "\r\n",
        "        self.market_value = self.stock_value + self.hold_money \r\n",
        "        self.total_profit = self.market_value - self.init_money\r\n",
        "        \r\n",
        "        reward = (self.trend[self.t + 1] - self.trend[self.t]) / self.trend[self.t]\r\n",
        "        if np.abs(reward)<=0.015:\r\n",
        "            self.reward = reward * 0.2\r\n",
        "        elif np.abs(reward)<=0.03:\r\n",
        "            self.reward = reward * 0.7\r\n",
        "        elif np.abs(reward)>=0.05:\r\n",
        "            if reward < 0 :\r\n",
        "                self.reward = (reward+0.05) * 0.1 - 0.05\r\n",
        "            else:\r\n",
        "                self.reward = (reward-0.05) * 0.1 + 0.05\r\n",
        "        \r\n",
        "        # reward = (self.trend[self.t + 1] - self.trend[self.t]) / self.trend[self.t]\r\n",
        "        if self.hold_num > 0 or action == 2:                                \r\n",
        "            self.reward = reward    \r\n",
        "            if action == 2:\r\n",
        "                self.reward = -self.reward\r\n",
        "        else:\r\n",
        "            self.reward = -self.reward * 0.1\r\n",
        "            # self.reward = 0\r\n",
        "        \r\n",
        "        self.last_value = self.market_value\r\n",
        "        self.market_value_list.append(self.market_value)\r\n",
        "        self.profit_rate_account.append((self.market_value - self.init_money) / self.init_money)\r\n",
        "        self.profit_rate_stock.append((self.trend[self.t] - self.trend[0]) / self.trend[0])\r\n",
        "        done = False\r\n",
        "        self.t = self.t + 1\r\n",
        "        if self.t == len(self.trend) - 2:\r\n",
        "            done = True\r\n",
        "        s_ = self.get_state(self.t)\r\n",
        "        reward = self.reward\r\n",
        "        \r\n",
        "        return s_, reward, done\r\n",
        "    \r\n",
        "    def get_info(self):\r\n",
        "        return self.states_sell, self.states_buy, self.profit_rate_account, self.profit_rate_stock  \r\n",
        "    \r\n",
        "    def draw(self):\r\n",
        "        # draw the result\r\n",
        "        states_sell, states_buy, profit_rate_account, profit_rate_stock = self.get_info()\r\n",
        "        invest = profit_rate_account[-1]\r\n",
        "        total_gains = self.total_profit\r\n",
        "        close = self.trend\r\n",
        "        fig = plt.figure(figsize = (15,5))\r\n",
        "        plt.plot(close, color='r', lw=2.)\r\n",
        "        plt.plot(close, 'v', markersize=8, color='k', label = 'selling signal', markevery = states_sell)\r\n",
        "        plt.plot(close, '^', markersize=8, color='m', label = 'buying signal', markevery = states_buy)        \r\n",
        "        plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\r\n",
        "        plt.legend()\r\n",
        "        #plt.savefig(save_name1)\r\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7SecGK-zc39"
      },
      "source": [
        "def game_step(env,agent,observation, step=None, train=True, show_log=False, my_trick=False):\r\n",
        "    \r\n",
        "    # RL choose action based on observation\r\n",
        "    action = agent.choose_action(observation, train)\r\n",
        "\r\n",
        "    # RL take action and get next observation and reward\r\n",
        "    observation_, reward, done = env.step(action, show_log=show_log, my_trick=my_trick)\r\n",
        "\r\n",
        "    agent.inventory(observation, action, reward, observation_)\r\n",
        "    # print(\"total profit:%.3f\" % env.total_profit, end='\\r')\r\n",
        "    if step and (step > 200) and (step % 5 == 0):\r\n",
        "      agent.train()\r\n",
        "\r\n",
        "    # swap observation\r\n",
        "    observation = observation_\r\n",
        "    \r\n",
        "    return observation, done\r\n",
        "    \r\n",
        "\r\n",
        "def run_DQN(env,agent,epcho, check_point):\r\n",
        "    step = 0\r\n",
        "    for episode in range(epcho):\r\n",
        "        # initial observation\r\n",
        "        observation = env.reset()\r\n",
        "\r\n",
        "        while True:\r\n",
        "            \r\n",
        "            observation, done = game_step(env, agent,observation, step=step)\r\n",
        "            # print(observation)\r\n",
        "            # break while loop when end of this episode\r\n",
        "            if done:\r\n",
        "                break\r\n",
        "            step += 1\r\n",
        "        if episode % check_point == 0:\r\n",
        "          print('epoch:%d, total_profit:%.3f' % (episode, env.total_profit))\r\n",
        "        # BackTest(False)\r\n",
        "\r\n",
        "\r\n",
        "def BackTest_DQN(env, agent,show_log=True, my_trick=False):\r\n",
        "    observation = env.reset()\r\n",
        "    # step=0\r\n",
        "    while True:\r\n",
        "        observation, done = game_step(env, agent,observation, train=False, \r\n",
        "                                      show_log=show_log, my_trick=my_trick)\r\n",
        "        # break while loop when end of this episode\r\n",
        "        if done:\r\n",
        "            break\r\n",
        "    print('total_profit:%.3f' % (env.total_profit))\r\n",
        "    return env\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No2KaL80zf1b"
      },
      "source": [
        "def back_test(df,initial_money,window_size,frequency,batch_size,iterations,checkpoint,gamma = 0.98,epsilon = 0.5, epsilon_min = 0.03,epsilon_decay = 0.999, lr =0.02): \r\n",
        "    trend = df.Close.values.tolist() #using close price to test  \r\n",
        "    \r\n",
        "    env_DQN = stock_env(df_copy.iloc[0:len(df_copy)-1])\r\n",
        "    agent_DQN = AgentDQN(env_DQN.n_actions, env_DQN.n_features,\r\n",
        "                      learning_rate= 0.01,\r\n",
        "                      reward_decay= epsilon_decay,\r\n",
        "                      e_greedy=gamma,\r\n",
        "                      replace_target_iter=200,\r\n",
        "                      memory_size=4000,\r\n",
        "                      batch_size=batch_size,                      \r\n",
        "                      )\r\n",
        "    run_DQN(env_DQN,agent_DQN, iterations,checkpoint)\r\n",
        "\r\n",
        "    env_DQN = stock_env(df_copy.iloc[0:len(df_copy)-1].reset_index(drop=True))  \r\n",
        "    env_DQN = BackTest_DQN(env_DQN, agent_DQN,show_log=True)\r\n",
        "   \r\n",
        "    env_DQN.draw()\r\n",
        "\r\n",
        "    # fig = plt.figure(figsize = (15,5))\r\n",
        "    # plt.plot(trend, color='r', lw=2.)\r\n",
        "    # plt.plot(trend, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\r\n",
        "    # plt.plot(trend, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\r\n",
        "    # plt.title('DQN gamma: %.3f,epsilon: %.3f,epsilon_min: %.3f,epsilon_decay: %.3f,lr: %.6f\\n'%(gamma,epsilon,epsilon_min,epsilon_decay,lr)+'total gains %f, total investment %f%%'%(total_gains, invest))\r\n",
        "    # plt.legend()\r\n",
        "    # plt.show()\r\n",
        "\r\n",
        "     #buy first day and hold strategy\r\n",
        "    buy_hold_strategy_value =  [] \r\n",
        "    amount_stock = initial_money/trend[0]\r\n",
        "    for price in trend : \r\n",
        "      buy_hold_strategy_value.append(amount_stock *price)\r\n",
        "    #print(len(buy_hold_strategy_value))\r\n",
        "    #print(len(agent.total_money_list))\r\n",
        "\r\n",
        "    fig = plt.figure(figsize = (15,5))\r\n",
        "    plt.plot(buy_hold_strategy_value, color='r', lw=2.,label=\"buy_hold_strategy%.1f\"%(buy_hold_strategy_value[-1]))\r\n",
        "    plt.plot(all_value_list_manual, color='y', lw=2.,label=\"Manual strategy%.1f\"%(all_value_list_manual[-1]))\r\n",
        "    #plt.plot(agent_Q.total_money_list, color='b', lw=2.,label=\"QL_agent_value%.1f\"%(agent_Q.total_money_list[-1]))\r\n",
        "    plt.plot(env_DQN.market_value_list, color='orange', lw=2.,label=\"DQN_agent_value%.1f\"%(env_DQN.market_value_list[-1]))\r\n",
        "    plt.plot(NQSDA_Index, color='g', lw=2.,label=\"NQSDA_value\" )\r\n",
        "    plt.plot()\r\n",
        "    plt.ylabel(\"total value\")\r\n",
        "    plt.xlabel(\"day\")\r\n",
        "    plt.legend()\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsuGGgXKzs7F"
      },
      "source": [
        "back_test(df,initial_money = 100000, window_size = 30, frequency = 7, batch_size = 32,iterations = 10, checkpoint = 10,gamma = 0.98,epsilon = 0.3, epsilon_min = 0.03,epsilon_decay = 0.999, lr =0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgD3vnanz0k1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}